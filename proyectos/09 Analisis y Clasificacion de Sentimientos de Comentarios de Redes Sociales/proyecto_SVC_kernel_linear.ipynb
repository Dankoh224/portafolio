{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828ef64f-6b36-4a41-a654-2ef3b085a523",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2> <b>DESAFÍO</b></h2>\n",
    "    <font size = \"5\">\n",
    "    <h1> <b>PROYECTO DE CLASIFICACIÓN DE TEXTO</b></h1><br>\n",
    "</div>\n",
    "<div>\n",
    "    <h3> <b>Algoritmos: Clasificación usando Máquinas de Soporte Vectorial con 3000 registros.</b></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdc8f33-0f9f-4603-9fa0-634990005395",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"emotion.jpg\"\n",
    "         alt=\"emociones de intensamente en el panel de control\"  width=\"650\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figura 1. Emociones queriendo ser transformadas en etiquetas mediante un modelo de Machine Learning. \n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7766417-3b0a-4bb0-b0ca-7b34b131ecef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# I° Parte: Preparación de los datos y creación del modelo de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5b1f63-1d1e-4a43-9f7b-48def6bb1f33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1) Importar librerías y dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5fbbe7-12e9-4dfa-9352-e389ddbaceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe de librerías de data análisis.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e534434-ac7b-4269-8bfb-9fe0cc163c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe de librerías de preprocesado y de machine learning.\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "196015c1-b771-4f88-aa7a-ac17c2175894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura del archivo train.csv.\n",
    "dataset = pd.read_csv('text_classification_train.csv')\n",
    "pd.set_option('display.max_columns',100) # ajuste para ver todas las columnas.\n",
    "pd.set_option('display.max_rows',10) # ajuste para ver todas las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24fab181-bb6e-464f-a6fb-cfe465026997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>27</td>\n",
       "      <td>eebbqej</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>27</td>\n",
       "      <td>ed00q6i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>2</td>\n",
       "      <td>eezlygj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>14</td>\n",
       "      <td>ed7ypvh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>3</td>\n",
       "      <td>ed0bdzj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43405</th>\n",
       "      <td>Added you mate well I’ve just got the bow and ...</td>\n",
       "      <td>18</td>\n",
       "      <td>edsb738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43406</th>\n",
       "      <td>Always thought that was funny but is it a refe...</td>\n",
       "      <td>6</td>\n",
       "      <td>ee7fdou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43407</th>\n",
       "      <td>What are you talking about? Anything bad that ...</td>\n",
       "      <td>3</td>\n",
       "      <td>efgbhks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43408</th>\n",
       "      <td>More like a baptism, with sexy results!</td>\n",
       "      <td>13</td>\n",
       "      <td>ed1naf8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43409</th>\n",
       "      <td>Enjoy the ride!</td>\n",
       "      <td>17</td>\n",
       "      <td>eecwmbq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43410 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text emotion       id\n",
       "0      My favourite food is anything I didn't have to...      27  eebbqej\n",
       "1      Now if he does off himself, everyone will thin...      27  ed00q6i\n",
       "2                         WHY THE FUCK IS BAYLESS ISOING       2  eezlygj\n",
       "3                            To make her feel threatened      14  ed7ypvh\n",
       "4                                 Dirty Southern Wankers       3  ed0bdzj\n",
       "...                                                  ...     ...      ...\n",
       "43405  Added you mate well I’ve just got the bow and ...      18  edsb738\n",
       "43406  Always thought that was funny but is it a refe...       6  ee7fdou\n",
       "43407  What are you talking about? Anything bad that ...       3  efgbhks\n",
       "43408            More like a baptism, with sexy results!      13  ed1naf8\n",
       "43409                                    Enjoy the ride!      17  eecwmbq\n",
       "\n",
       "[43410 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = dataset.drop(dataset.index[3000:]) # eliminé todos los registros a excepción de los 48 primeros.\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d4c178f-06b5-4542-8911-03038d6fdee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27           12823\n",
       "0             2710\n",
       "4             1873\n",
       "15            1857\n",
       "1             1652\n",
       "             ...  \n",
       "6,15,22          1\n",
       "9,10,19          1\n",
       "7,10,25          1\n",
       "7,9,24,25        1\n",
       "0,1,18           1\n",
       "Name: emotion, Length: 711, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frecuencia de las emociones predominantes.\n",
    "dataset['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f095a4-ce03-4dcf-860c-eefca17e0611",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2) Preprocesado del texto (eliminación del ruido en el texto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e429ee-c863-4d3b-89e6-72e217bac1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Danko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re # Importación de expresiones regulares para eliminar todo caracter distinto a letras.\n",
    "import nltk # importe de un kit con las principales palabras \"inútiles\" para el modelo.\n",
    "nltk.download('stopwords') # descarga de todas las palabras inservibles para el modelo.\n",
    "from nltk.corpus import stopwords # incorpora la descarga de palabras para utilizarlas.\n",
    "from nltk.stem.porter import PorterStemmer # permite transformar palabras a su mínima conjugación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75c461c-869d-4efd-a493-df50c1bae630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crearemos un corpus, una lista con las 43410 reseñas de texto, pero sin palabras \"ruido\" (conjunciones, artículos, etc).\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae72780e-4117-4fe8-aa34-489c66940503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucle de limpieza (rocorrerá cada dato de la columna \"text\").\n",
    "for i in range (0,43410):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['text'][i])\n",
    "    # Se crea la variable \"review\" que se transformará en un texto sin ruido para guardar en el corpus.\n",
    "    # Función sub: el primer parámetro indica que borraremos todo menos minúsculas y mayúsculas.\n",
    "    # El segundo parámetro indica qué carácter sustituiremos en el lugar de los carácteres borrados.  \n",
    "    # El tercer parámetro indica a qué elemento se le hará esta sustitución.\n",
    "    review = review.lower() # todo a minúsculas.\n",
    "    review = review.split() # de cadena de strings a lista de palabras.\n",
    "    ps = PorterStemmer() # objeto para transformar palabras a su mínima expresión (palabras sin conjugar).\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    # Este es un bucle que pasa por todas las palabras de la cadena \"i\" y mantiene la palabra si ésta\n",
    "    # no se encuentra en la lista de palabras inservibles que descargamos (las stopwords).\n",
    "    # Un detalle importante, dado que \"stopwords.words('english')\" es una lista, si queremos un\n",
    "    # código más rápido, rodearemos con la función \"set()\" para que la selección se transforme en un\n",
    "    # conjunto, y en vez de recorrer todos los elementos hasta encontrar la palabra como lo haría la lista,\n",
    "    # el conjunto hallará todas las palabras diferentes. Esto desarrollará un algoritmo mucho más rápido que pasará muchas\n",
    "    # menos veces por la comprobación.\n",
    "    # nota: Ps.stem no guarda la palabra tal y como se encontraba, sino su mínima conjugación.\n",
    "    review = ' '.join(review) # transformación de la lista ya filtrada, a cadena de texto.\n",
    "    corpus.append(review) # añade a la lista corpus cada variable review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5475dc9-2ca6-4102-ac89-f93d63ecbe9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3) Bag of Words (bolsa de palabras listas para el modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74a5e727-7491-4041-954c-69c2ef732f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer # transformación de cadena de palabras a vectores\n",
    "# de frecuencia. En la documentación se pueden observar muchas funciones, inclusive la automatización\n",
    "# de la limpieza que realicé anteriormente de forma manual.\n",
    "cv = CountVectorizer(max_features= 1500) # creamos el objeto CountVectoraizer.\n",
    "X = cv.fit_transform(corpus).toarray() # hacemos fit_transform y matriz dispersa. Filas son valoraciones, columnas son 0 y 1.\n",
    "# Es recomendable transformar a una matriz toarray, ya que se crearán muchas columnas.\n",
    "y = dataset.iloc[:,1] # ya que tenemos las X, obtenemos las y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f00ecd-3291-460e-997a-bb377860a8d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4) Transformación de la variable objetivo para el MultiLabelBinarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18564488-c1f4-456e-8956-359a49094fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función lambda con método split para cambiar los datos de string a lista (se usa la coma para separar cada etiqueta). \n",
    "dataset['emotion'] = dataset['emotion'].apply(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcbffed8-d167-44f3-a351-2016ffd28d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6', '9', '27']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Revisamos\n",
    "dataset['emotion'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d99d607e-51fa-40bf-b19e-2ad20719f5c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>[27]</td>\n",
       "      <td>eebbqej</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>[27]</td>\n",
       "      <td>ed00q6i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>[2]</td>\n",
       "      <td>eezlygj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>[14]</td>\n",
       "      <td>ed7ypvh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>[3]</td>\n",
       "      <td>ed0bdzj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...</td>\n",
       "      <td>[26]</td>\n",
       "      <td>edvnz26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yes I heard abt the f bombs! That has to be wh...</td>\n",
       "      <td>[15]</td>\n",
       "      <td>ee3b6wu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We need more boards and to create a bit more s...</td>\n",
       "      <td>[8, 20]</td>\n",
       "      <td>ef4qmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Damn youtube and outrage drama is super lucrat...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>ed8wbdn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It might be linked to the trust factor of your...</td>\n",
       "      <td>[27]</td>\n",
       "      <td>eczgv1o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion       id\n",
       "0  My favourite food is anything I didn't have to...     [27]  eebbqej\n",
       "1  Now if he does off himself, everyone will thin...     [27]  ed00q6i\n",
       "2                     WHY THE FUCK IS BAYLESS ISOING      [2]  eezlygj\n",
       "3                        To make her feel threatened     [14]  ed7ypvh\n",
       "4                             Dirty Southern Wankers      [3]  ed0bdzj\n",
       "5  OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...     [26]  edvnz26\n",
       "6  Yes I heard abt the f bombs! That has to be wh...     [15]  ee3b6wu\n",
       "7  We need more boards and to create a bit more s...  [8, 20]  ef4qmod\n",
       "8  Damn youtube and outrage drama is super lucrat...      [0]  ed8wbdn\n",
       "9  It might be linked to the trust factor of your...     [27]  eczgv1o"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55f8b20-4c46-490d-b57f-1af0db14188e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5) Multi-label binarizer (matriz dispersa para las variables objetivos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64334bed-dad3-4dd0-beb4-c27281f54cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel = MultiLabelBinarizer() # creación del objeto para crear la matriz dispersa a partir del texto \n",
    "y = multilabel.fit_transform(dataset['emotion']) # ajuste y transformación del modelo\n",
    "# Revisamos\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be34e34e-9641-4021-8b44-e7804442ed8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18',\n",
       "       '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '3',\n",
       "       '4', '5', '6', '7', '8', '9'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel.classes_ # método para obtener las etiquetas de cada emoción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a3a08b8-a9d9-461e-a8a4-726b16aa5a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>2</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  10  11  12  13  14  15  16  17  18  19  2  20  21  22  23  24  25  \\\n",
       "0  0  0   0   0   0   0   0   0   0   0   0   0  0   0   0   0   0   0   0   \n",
       "1  0  0   0   0   0   0   0   0   0   0   0   0  0   0   0   0   0   0   0   \n",
       "2  0  0   0   0   0   0   0   0   0   0   0   0  1   0   0   0   0   0   0   \n",
       "3  0  0   0   0   0   0   1   0   0   0   0   0  0   0   0   0   0   0   0   \n",
       "4  0  0   0   0   0   0   0   0   0   0   0   0  0   0   0   0   0   0   0   \n",
       "5  0  0   0   0   0   0   0   0   0   0   0   0  0   0   0   0   0   0   0   \n",
       "6  0  0   0   0   0   0   0   1   0   0   0   0  0   0   0   0   0   0   0   \n",
       "7  0  0   0   0   0   0   0   0   0   0   0   0  0   1   0   0   0   0   0   \n",
       "8  1  0   0   0   0   0   0   0   0   0   0   0  0   0   0   0   0   0   0   \n",
       "9  0  0   0   0   0   0   0   0   0   0   0   0  0   0   0   0   0   0   0   \n",
       "\n",
       "   26  27  3  4  5  6  7  8  9  \n",
       "0   0   1  0  0  0  0  0  0  0  \n",
       "1   0   1  0  0  0  0  0  0  0  \n",
       "2   0   0  0  0  0  0  0  0  0  \n",
       "3   0   0  0  0  0  0  0  0  0  \n",
       "4   0   0  1  0  0  0  0  0  0  \n",
       "5   1   0  0  0  0  0  0  0  0  \n",
       "6   0   0  0  0  0  0  0  0  0  \n",
       "7   0   0  0  0  0  0  0  1  0  \n",
       "8   0   0  0  0  0  0  0  0  0  \n",
       "9   0   1  0  0  0  0  0  0  0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y, columns = multilabel.classes_).head(10) # dataframe para ver la matriz con sus respectivas etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b7e9706-5088-459a-8b7e-bd0210b23162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43410, 1500), (43410, 28))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape , y.shape # shape para ver la cantidad de filas y columnas de cada matriz de dispersión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab47875b-0d5e-45ba-a67b-d992dcab6677",
   "metadata": {},
   "source": [
    "### Ya están listas las variables predictoras y las variables objetivo para comenzar la fase de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b613f8-c0c4-4f65-b07b-4a517fd2226b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 6) Fase de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97826b6f-88c1-49da-a80a-3629b06979dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de las variables en variables de entrenamiento (80%) y variables de testeo (20%).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c9797d0-246f-48d2-8bda-b42870d71b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34728, 28)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape # shape para ver la cantidad de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c06e5d8-81ac-49c1-afb8-90f8840a2906",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 7) Modelo SVC (máquina de vectores de soporte C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b34d91-4d85-4ea0-9118-479f17778c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier # Importe de algoritmo de clasificación multiclase.\n",
    "from sklearn.svm import SVC # importe del modelo de clasificación de vectores de soporte C.\n",
    "\n",
    "clf = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "# Creamos el objeto OneVsRestClassifier y le asignamos como parámetro el modelo SVC. Este estimador utiliza el método de\n",
    "# relevancia binaria para realizar las clasificaciones multietiqueta, entrenando un clasificador binario independiente para\n",
    "# cada etiqueta.\n",
    "clf.fit(X_train, y_train) # ajuste de estimadores subyacentes (datos de entrenamiento X e y).\n",
    "\n",
    "y_pred = clf.predict(X_test) # predicción de objetivos de varias etiquetas usando el 20% de los datos X de testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189c61e2-3507-4a1e-a075-0f4cfab3171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(y_pred, columns = multilabel.classes_).head(10) # dataframe de y_pred con las respectivas etiquetas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f1257-06c8-4f16-ab9c-dafc8b848067",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 8) Evaluación del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe220a0-f128-4b06-ad94-d27773fa6d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importe de librerías para medir el accuracy, la matriz de confusión y otras métricas de precisión.\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f2d6a-1cf8-4587-8ee5-fb06355b13dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El método score devuelve la precisión media en los datos de prueba y las etiquetas dadas.\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62daac-8299-42ba-bc40-ef5878bc27ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasa de acierto\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1b87f-f9ce-4759-8744-62592e237fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe de matriz de confusión multietiqueta\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "multilabel_confusion_matrix(y_test, y_pred) # imprime matrices de confusión para las predicciones de todas las etiquetas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01255d9-3f20-4048-ac5b-ffaa18548f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimiremos la precisión, el recall y el F1-score para las predicciones de cada una de las etiquetas.\n",
    "# Pero antes, cambiaremos el nombre de las etiquetas (nombre de la emoción). Crearemos una lista llamada\n",
    "# label_names, y la entregaremos como parámetro a la función classification_report. El orden no es el presentado en el archivo\n",
    "# emotion.txt, si no el que entregó el MultiLabelBinarizer.\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11e0e75-610b-4787-90fb-fe117a7a98a6",
   "metadata": {},
   "source": [
    "### Evaluación y Conclusión.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15158538-e5ae-4238-a2f7-81e9f1bbfd64",
   "metadata": {},
   "source": [
    "### 1.- ACCURACY: La tasa de acierto de la función \"accuracy_score\" es de un 36%, sin embargo, se utiliza una matriz de confusión multietiqueta para identificar cuáles son las emociones con más alto accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e42eae6-69c8-4f6c-9eea-348b1f1e5071",
   "metadata": {},
   "source": [
    "### 2.- PRECISIÓN: La calidad del etiquetado que predijo el modelo SVC, fue bastante bueno. Al menos 24 de las 28 emociones, fueron predichas con una precisión igual o superior 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc39ee-d769-4dd3-b46e-1fc0e2973cce",
   "metadata": {},
   "source": [
    "### 3.- RECALL: La cantidad de etiquetas positivas que el modelo logró identificar correctamente, en general, fue muy bajo. El modelo solo logró identificar 5 etiquetas con un porcentaje igual o superior al 50%. El mejor recall fue el de la etiqueta #15, correspondiente a la emoción de gratitud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b745d83-8012-4580-81b4-0ca0ace20fa6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.- F1-SCORE: Utilizando un margen superior al 50%, las etiquetas con mejor F1_score (media armónica entre precisión y recall) son 8 emociones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0cca2-0001-44a6-84a4-0d5354a3e71c",
   "metadata": {},
   "source": [
    "### CONCLUSIÓN.\n",
    "\n",
    "### El modelo SVC para clasificación multietiqueta, presenta un nivel de ACCURACY bajísimo, por lo que se observa con minuciosidad, la PRECISIÓN Y EL RECALL de los modelos. \n",
    "\n",
    "### Como resultado, podemos ver una PRECISIÓN muy alta del etiquetado de cada emoción, como por ejemplo en la \"gratitud\" con un 93% de precisión y en la \"curiosidad\", con un 100% de aciertos. Sin embargo, se observa un RECALL bajísimo en comparación con los valores obtenidos en la métrica anterior, y claro, al ver las matrices de confusión, se puede observar que en la emoción \"curiosidad\", por ejemplo, predice 18 de las 18 predicciones realizadas, sin embargo, le faltaron 417 comentarios que etiquetar con esta emoción.\n",
    "\n",
    "### Se realizan dos conclusiones:\n",
    "\n",
    "### 1) El modelo fue bastante tímido para realizar predicciones. Podríamos decir que el modelo se atrevió a predecir solo cuando estaba muy seguro de acertar. Esto se evidencia por el número de comentarios no etiquetados (ESCRIBIR EL NÚMERO AQUÍ), lo que se resume en un (ESCRIBIR EL NÚMERO AQUÍ)% de comentarios con su contenido desconocido.\n",
    "\n",
    "### 2) El etiquetado es, en gran medida muy preciso, sin embargo, en algunas etiquetas es altamente confiable y en otras lo es nulamente, por lo que hay que es imprecindible realizar una lista de las emociones más confiables, si lo que se desea es realizar KPIs fidedignas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90426f0f-bf61-4255-91e2-12096138fcc9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# II° Parte: Clasificación final del dataset \"text_classification_test\" (predicción)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8dd317-0d32-4b94-9cb6-f1c01b3b1517",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1) Lectura de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b11f82-65d2-4b87-9467-56d383725ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text_classification = pd.read_csv('text_classification_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a396f-6d12-4f92-b2eb-2876848b7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927503e2-f857-446e-86c1-1b2986af68e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2) Preprocesado de las reseñas del csv de testing (limpieza)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323540c4-98ef-4c49-a87a-3c493623d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus para la predicción\n",
    "corpus_text_classification = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624a290-2107-47fc-9286-02e588e1ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucle de limpieza\n",
    "for i in range (0,5427):\n",
    "    review_text_classification = re.sub('[^a-zA-Z]', ' ', dataset_text_classification['text'][i])\n",
    "    review_text_classification = review_text_classification.lower()\n",
    "    review_text_classification = review_text_classification.split()\n",
    "    ps_text_classification = PorterStemmer()\n",
    "    review_text_classification = [ps_text_classification.stem(word) for word in review_text_classification if not word in set(stopwords.words('english'))]\n",
    "    review_text_classification = ' '.join(review_text_classification)\n",
    "    corpus_text_classification.append(review_text_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad7f10-84af-464a-b3b5-07093698cc4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3) Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcb1456-5d97-4565-b836-182053489dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_text_classification = CountVectorizer(max_features= 1500)\n",
    "X_text_classification = cv_text_classification.fit_transform(corpus_text_classification).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2b0f3-2ac3-42c2-81a7-c99d1e261c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59453d3b-24b9-4bfd-87d8-80eda764b93b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4) Nube de puntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b778a3bc-85a0-43d7-b035-4b052a9e50e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud # importamos librería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd67d9b-31c1-4e34-a955-e0bb686dceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el objeto. Fondo negro, alto y ancho.\n",
    "wc_text_classification = WordCloud(background_color='black', height=600, width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a674e599-8980-4e58-aaf8-44bc1584d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud solo acepta texto, así que se transforma la lista de strings (corpus) en un string.\n",
    "corpus_wc = ''\n",
    "for i in corpus_text_classification:\n",
    "    corpus_wc = corpus_wc + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ac6e2-d714-438c-88a2-b44f6d6404f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se genera el wordcloud entregándo como argumento el corpus_wc \n",
    "wc_text_classification.generate(corpus_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1753c98d-be41-4a4f-821b-d4e0f8a9338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se genera un archivo png para insertar una imagen en la próxima celda. Queda muy bonita.\n",
    "wc_text_classification.to_file('wordcloud_text_classification.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c91e57-32cd-45e8-8c25-ef4e88cca5f6",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"wordcloud_text_classification.png\" alt=\"Nube de Palabras\"  width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81839f1d-4695-4ee6-a697-a42178ccab4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    " ## 5) Predicción usando nuestro modelo de máquinas de soporte vectorial.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c8615-e4e2-4168-bf8d-94ccc847645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_text_classification = clf.predict(X_text_classification) # realizamos la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616b3bb1-a5b3-419b-b5c7-44c542380e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred_text_classification, columns = multilabel.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c954f618-4371-45fe-ab40-5c72ad53fd39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 6) Transformación Inversa (transformar matriz dispersa a columna de etiquetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956b45e-be53-4382-baea-cebbf33a3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos el método \"inverse_transform\" de la librería MultiLabelBinarizer para volver a tener el nombre de nuestras\n",
    "# etiquetas (números del 0 al 27).\n",
    "y_pred_transformado = multilabel.inverse_transform(y_pred_text_classification) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ea713a-f104-4dd2-a0c7-fd7fc5f93064",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 7) Agregar las predicciones como columna al dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c719a7c-d589-47e3-b03b-0a17a8659b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text_classification['Emoción'] = y_pred_transformado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbe680-c39c-45f9-a648-0aee82689d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc34a42-e79d-408b-ab7a-7e8e9377c5d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8) Transformación de la columna Emoción a str con la función lambda y el método join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf272d3-fd2d-4616-bd64-605bda21993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text_classification['Emoción'] = dataset_text_classification['Emoción'].apply(lambda x: ','.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1878be-4fb7-48cc-a927-02a73609a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text_classification['Emoción'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51baeda2-7a99-49ad-8902-ff6c4f5e864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset_text_classification['Emoción'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd4e4e-eefe-45f0-8b9a-55210580153c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# III° Parte: KPIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3372e734-4575-4207-bed1-dd9cc3467a94",
   "metadata": {
    "tags": []
   },
   "source": [
    "## KPI\n",
    "### En base a la clasificación de los comentarios, realizaremos un KPI capaz de medir el nivel de satisfacción global (explicar significado y justificar selección). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17aaeb-cb5b-4d37-8b4b-11c5484c0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para realizar un KPI valioso, veremos el nivel de comentarios positivos, negativos y neutros,\n",
    "# que hay en nuestro dataset.\n",
    "dataset_text_classification['Emoción'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbfba3a-e002-4a75-a0ee-059382f1d2be",
   "metadata": {},
   "source": [
    "### Observemos que hay 1895 datos sin clasificar, lo que significa que en este caso, el modelo etiquetó el 65% de los comentarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100449c-c4be-4a38-a304-28e1667ab945",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',10) # ajuste para ver todas las columnas.\n",
    "dataset_text_classification['Emoción'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1532fb-cb0a-452b-9bf0-93eb4db98e92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# IV° Parte: Post-desarrollo (zona de preguntas y respuestas). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a54c8f-8ea7-4da4-b6f2-41f553cc68aa",
   "metadata": {},
   "source": [
    "### 1.\t¿De qué manera se puede complementar la solución? Pensar en propuestas para el cliente.\n",
    "#### Una forma de complementar la solución, sería aumentando el valor del modelo de clasificación de texto. Esto quiere decir, alojar los modelos de machine learning en un sistema web en la nube, que, a pesar de requerir un esfuerzo relativo mayor, entregaría muchas más funcionalidades. Por ejemplo, la posibilidad de permitir al usuario elegir el modelo a utilizar, conociendo a priori el tiempo de ejecución, certeza y el porcentaje de etiquetado que entrega cada modelo, la posibilidad de realizar una cascada de los datos a través de los modelos, comparar entre los resultados de los modelos o acceder a funcionalidades de visualización de resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8654770-e0d4-4e23-8ad4-b27fe7381f18",
   "metadata": {},
   "source": [
    "### 2.\t¿Cómo se podría simplificar la tarea?\n",
    "#### El modelo podría simplificarse eludiendo el preprocesado de texto, etapa que requiere de mucho tiempo si el proceso se realiza con grandes cantidades de datos. Sin embargo, la cantidad de tiempo que demora el entrenamiento, sería mayor, y probablemente, los resultados serían menos exactos. Otra forma podría ser no utilizar multietiquetado, para no confundir al modelo (solo trabajar con clases), o bien, reduciendo la cantidad de etiquetas, lo que por un lado permitiría mejores niveles de asertividad, lo que lamentablemente haría perder el nivel de detalle de emociones que presenta el dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd1f2d4-2443-4ed6-ad75-317c0e2f4715",
   "metadata": {},
   "source": [
    "### 3.\t¿Cuáles pueden ser las limitaciones, riesgos, sesgos de los modelos al implementar este tipo de soluciones? \n",
    "#### Las limitaciones de estos modelos pasan por ser soluciones generalmente costosas y que pueden llevar mucho tiempo dada la binarización de los datos tipo texto y el multietiquetado. En este caso en particular, además, el multietiquetado contenía 28 valores distintos, por lo que la cantidad de veces que el código es aplicado, aumenta. Por otra parte, estos modelos presentan dos situaciones que aumentan el riesgo de su uso; 1) Los modelos no clasifican a todos los comentarios, por ende, se estarían tomando decisiones sin la totalidad de los datos. 2) El sesgo que puede presentar este modelo,  ya que pueden existir errores en los datos, cosa que se infiere en el enunciado del desafío: “La empresa ya ha clasificado manualmente alrededor de 48 comentarios, con 28 emociones diferentes”, sin embargo, un poco más arriba, la empresa evidencia que “Actualmente, los comentarios se clasifican manualmente, lo que es un proceso lento y propenso a errores”, lo que podría conllevar a que el resultado final, necesariamente presente sesgo (error en la captura de datos). Por otra parte, también puede ser posible que estos datos presenten sesgo algorítmico, sesgo que afecta principalmente a las menorías o a aquellos grupos de datos que no están bien representados. Los modelos pueden basarse en gran medida en etiquetas que están correlacionados con otras etiquetas y asignar un mal etiquetado a un comentario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf32e1-7d6f-4c37-be6d-2b986ac732c0",
   "metadata": {},
   "source": [
    "### 4.\t¿Qué otras cosas hay que considerar al momento de implementar un proyecto como este?\n",
    "#### Primordial en primer lugar es realizar una buena preparación de los datos para que no tengamos que descartar el modelo por malos resultados. Es importante eliminar ciertos caracteres y palabras clave (stopwords). Además, es importante utilizar diferentes funciones para algunos procesos, como por ejemplo CountVectorizer vs TfidfVectorizer, para elegir la que mejor se ajusta a los datos de entrenamiento (en este caso, TfidfVectorizer emplea ponderación inversa, es decir, el modelo otorga menor ponderación a las palabras con menor frecuencia, aprendiendo que estas tienen menor importancia). Por otra parte, también es necesario practicar con diferentes modelos de machine learning y jugar con sus parámetros, para ver qué algoritmo se ajusta mejor a los requerimientos del cliente (mayor velocidad, mayor precisión o mayor exhaustividad).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce79b683-b618-4a5d-a8f5-7d81e5327559",
   "metadata": {
    "tags": []
   },
   "source": [
    "# V° Parte: Entregable (exportar dataset con columna de predicciones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b09e20e-c73c-4de3-8b67-46fa32011b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text_classification.to_csv('text_classification_entrega.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000609e-89ca-4177-92c8-ab1241a845d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
